---
title: "Assignment 6 Nextech Case"
author: "Sebastian Perez Parra"
date: "2025-09-14"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true
  error: false    
  warning: false
  message: false
---

We start by loading the necessary libraries and the dataset.

```{r load libraries}
#load libraries
library(tidyverse)
library(rpact)
library(MatchIt)
library(marginaleffects)
```

```{r load dataset}
#load historical data
d <- read_csv("https://raw.githubusercontent.com/jefftwebb/data/main/management_training.csv")
```

# 1

We begin by estimating the average treatment effect (ATE) by calculating the difference in mean engagement scores for treatment vs. control.

```{r ARPU}
mean(filter(d, intervention == 1)$engagement_score) - mean(filter(d, intervention == 0)$engagement_score)
```

or using regression. Either way will give us the same result.

```{r}
summary(lm(engagement_score ~ intervention, data  = d))
```

-   

# 2

Now we Estimate ATE with multiple regression, adjusting for possible confounders. We include all the variables except for `department_id` which is just an identifier.

```{r}
summary(lm(engagement_score ~ intervention + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size, data = d))
```

-   The naive effect of the treatment show a great positive effect of the management training where the treatment showed an improvement of 0.43 in the engagement score. The effect with the regression is statistically significant but shows a decrease in the effect, estimating only 0.27 increase in engagement scores. This suggests that the naive effect is biased upward increasing the treatment effect.

The large naive effect is very possibly biased with selection. Since the data is historical observed data that is non compliant because some managers assigned to the training did not attend, while others attended without having been assigned, self selection was introduced. This eliminates any randomization. This difference could also be a hint of potential confounders, pre existing differences in the data that affect the treatment and the outcome.

# 3

We use coarsened exact matching (CEM) to create synthetic treatment and control groups for estimating the treatment effect and estimate the treatment effect (ATM) using the matched sample.

First, we do the matching. Intervention is the dependent variable in the formula and outcome is left out because we are *modeling treatment assignment*. This creates a matchit object.

```{r}
m_cem <- matchit(formula = intervention ~ department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size,
                 data = d,
                 cutpoints = "sturges",
                 # you could try = "sturges" for automatic bin selection
                 method = "cem")

m_cem
```

Second, we check the matches.

```{r}
summary(m_cem)
```

Third, we plot the matches to evaluate improvement. The goal is minimize treatment/control differences in the matched sample.

```{r}
plot(summary(m_cem))
```

In general this is an improvement: the matched sample variables are moving towards 0.

Fourth, we construct the matched data set. This consists in a subset of only the matched units.

```{r}
(matched_cem <- match.data(m_cem))
```

Finally, we do the analysis using the matched data.

```{r}
lm(engagement_score ~ intervention + department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size, 
   weights = weights, # this is necessary because of number differences in subclass
   data = matched_cem) |>
  summary()
```

-   The estimated treatment effect (ATM) using the matched sample is 0.22. That is, 0.22 more for the engagement score in the treatment than the control.

# 4

Calculate and summarize propensity scores for the training program.

```{r}
# 1. Estimate propensity scores
ps_model <- glm(intervention ~ department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size, 
                family = "binomial", 
                data = d) 

# 2. Predict probabilities.  Use type = "response".
d$ps <- predict(ps_model, type = "response") 
  
# 3. Calculate IPW
d <- d |>
  mutate(ipw = ifelse(intervention == 1, 1/ps, 1/(1 - ps)))

# 4. Check overlap of propensity scores for trimming
ggplot(d, aes(ps)) +
  geom_histogram() +
  facet_wrap(~intervention, ncol = 1)

# or 

summary(filter(d, intervention == 1)$ps)
summary(filter(d, intervention == 0)$ps)
```

-   The best cut offs for trimming the data to ensure overlap in propensity scores between treatment and control prior to doing an analysis with inverse propensity score weighting (IPW) are 0.1 and 0.78

# 5

We use IPW in regression to estimate the ATE of the training program.

```{r}
lm(engagement_score ~ intervention + department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size,  
   weights = ipw, 
   data = filter(d, ps > .1 & ps < .78)) |>
  summary()
```

The estimated ATE is 0.27.

Do the bootstrap analysis

```{r}

# initialize the vector of simulated estimates
boot_dist <- NA

for(i in 1:200){

  # initialize the bootstrap sample
  boot_samp <- slice_sample(.data = d, prop = 1, replace = T) # check ?slice_sample

  # Estimate propensity scores
  ps_model <- glm(intervention ~ department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size, 
                family = "binomial", 
                data = boot_samp) # use the bootstrap sample here
  
  # Predict probabilities.  Use type = "response".
  boot_samp$ps <- predict(ps_model, type = "response") 
  
  # Calculate IPW
  boot_samp <- boot_samp |>
    mutate(ipw = ifelse(intervention == 1, 1/ps, 1/(1 - ps)))
  
  # Estimate effects using ipw as weights but trim out the lowest and highest ps
  fit <- lm(engagement_score ~ intervention * department_id * tenure * n_of_reports * gender * role * last_engagement_score * department_score * department_size,  
            weights = ipw, 
            data = filter(boot_samp, ps > .05 & ps < .95))  # make a guess about trimming
  
  # Store estimated ATE
  boot_dist[i] <- avg_comparisons(fit, 
                                variables = "intervention",
                                wts = "ipw")$estimate 

   #print(i) # make sure to comment this out before compiling!
}

# non-parametric bootstrap estimate:
quantile(boot_dist, probs = c(.025, .5, .975), na.rm = TRUE) 

# There is also a parametric bootstrap that uses SD as SE
se <- sd(boot_dist)
mean(boot_dist) - 1.96 * se # lower bound
mean(boot_dist) + 1.96 * se # upper bound


```

# 6

We use full matching to create weighted synthetic treatment and control groups and we estimate ATE using G-Computation in the `marginaleffects` package.

```{r}
# 1. Create the matchit object with glm as the (default) method for estimating propensity scores
m_full <- matchit(formula = intervention ~ department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size,
                 data = d,
                 method = "full") # method = "quick" is an optimized version of "full"

# 2. Evaluate balance
summary(m_full)

plot(summary(m_full))

# 3. Create the matched data
(matched_full <- match.data(m_full))

# 4. Do the analysis using the weights that have been added to the matched data
model <- lm(engagement_score ~ intervention + department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size,  
   data = matched_full,  
   weights = weights)
  
summary(model)
```

Use marginal effects package to back out the treatment effect from this complicated model.

```{r}
avg_comparisons(model,
                variables = "intervention", #  identify the treatment variable
                wts = "weights")$estimate # extract the estimate
```

Using bootstrapping

```{r}
# initialize the vector of simulated estimates
boot_dist <- NA

for(i in 1:100){

  # initialize the bootstrap sample
  boot_samp1 <- slice_sample(.data = d, prop = 1, replace = T) # check ?slice_sample

  # Create the Matchit object
  full_m <- matchit(formula = intervention ~ department_id + tenure + n_of_reports + gender + role + last_engagement_score + department_score + department_size,
                 data = boot_samp1,
                 method = "full")

# Create the matched data
(matched_full <- match.data(full_m))
  
  # Estimate effects
  fit1 <- lm(engagement_score ~ intervention * department_id * tenure * n_of_reports * gender * role * last_engagement_score * department_score * department_size,
             data = matched_full,
            weights = weights)
  
  # Store estimated ATE
  boot_dist[i] <- avg_comparisons(fit1, 
                                variables = "intervention",
                                wts = "weights")$estimate 

   #print(i) # make sure to comment this out before compiling!
}

# We calculate the non-parametric bootstrap estimate:
quantile(boot_dist, probs = c(.025, .5, .975), na.rm = TRUE) 

# There is also a parametric bootstrap that uses SD as SE
se <- sd(boot_dist)
mean(boot_dist) - 1.96 * se # lower bound
mean(boot_dist) + 1.96 * se # upper bound
```

```{r}

boot_gcomp <- NA

for(i in 1:100){

  # Initialize the bootstrap sample
  boot_samp <- slice_sample(.data = d, prop = 1, replace = T) 
  

  # Full matching
  m_boot <- matchit(formula = intervention ~ tenure + n_of_reports + gender + role + 
                    last_engagement_score + department_score + department_size,
                   data = boot_samp,
                   method = "full")
  
  matched_boot <- match.data(m_boot)

# Complex outcome model
  model_boot <- lm(engagement_score ~ intervention * department_id * tenure * n_of_reports * gender * role * last_engagement_score * department_score * department_size,  
                   data = matched_boot,  
                   weights = weights) 
  
  # Store estimated ATE using G-computation
  boot_gcomp[i] <- avg_comparisons(model_boot, 
                                variables = "intervention",
                                wts = "weights")$estimate 

  # print(i) # comment out before compiling!
}

# Non-parametric bootstrap estimate:
quantile(boot_gcomp, probs = c(.025, .5, .975), na.rm = TRUE) 


# There is also a parametric bootstrap that uses SD as SE
se <- sd(boot_dist)
mean(boot_dist) - 1.96 * se # lower bound
mean(boot_dist) + 1.96 * se # upper bound

```

# 7

Nextech launched a managerial training program to improve team engagement, but participation was non compliant raising concerns about confounding. Managers who opted into training may differ systematically from those who didnâ€™t, in ways that also affect engagement outcomes. Key confounders include tenure, prior engagement scores, team size, department-level climate, and demographic factors like role and gender. These variables most probably influence both the treatment and the engagement scores we aim to measure.
