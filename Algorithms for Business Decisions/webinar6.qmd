---
title: "Class 6"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---


```{r}
library(tidyverse)
library(MatchIt) # Library for matching in R: Exact, CEM, Full Matching
library(marginaleffects)
```

See above for an example yaml header. Many people are using headers borrowed from RMarkdown,but in Quarto which lack crucial elements (e.g., embed-resources) for compiling.

# Fluent.io

## Q1

Use the historical data to calculate ARPU and SDRPU. 

```{r}
library(tidyverse)

d <- read.csv("https://raw.githubusercontent.com/jefftwebb/data/refs/heads/main/fluent_historical.csv")

# Calculate monthly ARPU-- equal weighting by customer month
d %>%
  mutate(month = month(date)) |>
  group_by(customer_id, month) |>
  summarize(revenue = sum(paid), .groups = "drop") |>
  summarize(arpu = mean(revenue),
            sdrpu = sd(revenue))

# Calculate monthly ARPU-- equal weighting by month
d %>%
  mutate(month = month(date)) |>
  group_by(customer_id, month) |>
  summarize(revenue = sum(paid), .groups = "drop") |>
  group_by(month) |>
  summarize(monthly_arpu = mean(revenue),
            monthly_sdrpu = sd(revenue), .groups = "drop") |>
  summarize(arpu = mean(monthly_arpu),
            sdrpu = mean(monthly_sdrpu))
```

## Q2

What is the total sample size required for a traditional fixed duration A/B test, assuming ARPU of 3.00, MEI of .50 and SDRPU of 11.00?

```{r}
n <- power.t.test(delta = .5, 
                  sd = 11, 
                  sig.level = .05, 
                  power = .8, 
                  alternative = "one.sided")$n

2 * ceiling(n) # = 11972

```

## Q3

What is the required duration in days for this A/B test.

```{r}
(new <- d |>
  filter(type == "subscription") |>
  arrange(customer_id, date) |>
  group_by(customer_id) |>
  summarize(customer_id = first(customer_id),
            date = first(date)) |>
  count(date) |>
  summarize(mean = mean(n)) |>
  pull(mean))

(2 * ceiling(n)) / new
```


## Q4 

CUPED?

## Q5

Size this experiment for a group sequential test with 4 checkpoints. 

```{r}

 library(rpact)

design <- getDesignGroupSequential(typeOfDesign = "asOF", 
                                   kMax = 4,
                                   alpha = 0.05, 
                                   typeBetaSpending = "bsOF") 

getSampleSizeMeans(design, 
                   alternative = 0.5, 
                   stDev = 11) |>
    summary()
```

## Q6

Suppose that the true effect of the treatment is .25. What is the probability given this group sequential design that the experiment could be stopped early?

This question was poorly framed.  I should have said:  suppose the treatment effect is 0.  To get exact early stopping probabilities for MEI of .25 we would need to update the design.
    
# Model dependence

Let's start by simulating some data to work with.  This is useful for exploring techniques for estimating treatment effects because we define the effect. It gives us an objective yardstick for comparing the success with which different methods recover the (known) treatment effect. 

Remember: A confounder both causes treatment assignment (Treatment) and causes the outcome (Y).

There are lots of ways to simulate confounded data.  The key is to give the confounder a role in the creation of both Treatment and Y. 

```{r}

make_confounded_data <- function(seed = 123, n = 1000, p = 5, tau = .5){ 
  
  # p is the number of predictors, minimum of 5.
  # tau is the defined treatment effect
  # The function outputs:
  # - A predictor matrix, X, including a confounder
  # - A treatment variable: Treatment
  # - An outcome variable: Y
  
  set.seed(seed)
  
  # 1. Generate the X predictor matrix from  N(0, 1.5).  
  #.   The choice of distribution is somewhat arbitrary.
  X <- matrix(rnorm(n * p, sd = 1.5), nrow = n)
  
  # 2. Define the confounder, W, to be the first column in X.
  W <- X[, 1]
  
  # 3. Generate treatment assignment probabilities from X1 using the inverse logit function.
  #    Essentially this converts W into a propensity score for use in treatment assignment.
  #    Note that plogis(W) would also work, generating a probability from the logistic distribution.
  propensity_score <- 1 / (1 + exp(-W)) 
  
  # 4. Generate random treatment assignment from a binomial distribution with prob = propensity_score.
  #.   Note: this ensures that W influences treatment assignment.
  Treatment <- rbinom(n, size = 1, prob = propensity_score)

  # 5. Generate potential outcomes, with and without treatment, for each observation. 
  #    The treatment effect (tau) is added to the treated potential outcome.
  #    The key here is that both potential outcomes depend on the confounder W.
  po_1 <- W + X[, 2] * X[, 5] + tau 
  po_0 <- W + X[, 3]* X[,4]

  # 6. Generate a continuous Y by selecting one of the potential outcomes based on (random) 
  #    treatment assignment. We additionally add in some noise with rnorm(). 
  Y <- po_0 * (1 - Treatment) + po_1 * Treatment + rnorm(n)

  # 7.  Create the data set with Y, Treatment, W, X.
  #     Note that the confounder has been included separately from X to facilitate estimation
  data <- data.frame(Y, Treatment, `X.0` = W, X = X[, 2:p])

}

(d <- make_confounded_data())

```

The naive estimate of the treatment effect involves treating the data as if it were experimental data and simply comparing means.

```{r}
# The estimated treatment effect is the coefficient for Treatment

lm(Y ~ Treatment, data = d) |> 
  summary()

# Or, alternatively:

mean(filter(d, Treatment == 1)$Y) - mean(filter(d, Treatment == 0)$Y)

```

The estimate is terrible.  We set tau to be .5.  

But:  what should the model be?

```{r}
lm(Y ~ ., data = d) |> 
  summary()
```

Or:  

```{r}
lm(Y ~  Treatment + (X.0 * X.1 * X.2 * X.3 * X.4), data = d) |> 
  summary()
```

In the real world we never know which model is correct (though we tend to rely on the idea that simpler is better). Model misspecification is always a danger in model building.  Results vary and depend on the exact model used. 

One particular danger is intentionally trying different models with the goal of finding a statistically significant effect.  This is p-hacking.  But it is easy to get model-dependent results mistakenly, with the best of intentions, since in real life, as mentioned, we never really know which model is correct, and there can be a great deal of variation in results.
 
## Matching

**Matching solves model dependency.**

The rationale for matching is that we separate: 

1. the activity of balancing the groups (which is what adding predictors to a model is doing) 
2. the activity of estimating the treatment effect. 

This eliminates model dependence and the potential for p-hacking because matching is carried out in the absence of the outcome variable.

This demonstration will use the MatchIt library in R, developed by the statistician Gary King and colleagues. 

There is a PyMatch library in python.  I am not sure how well it works. One thing you might consider if you want to complete most of required programming in Python is to use a nextgen IDE like Positron that would allow you to easily switch back and forth between Python and R. 

### Exact matching

Not possible.  Why?

### Coarsened Exact Matching (CEM)

Here are the steps for CEM:

1. Do the matching.  Notice that Treatment is the dependent variable in the formula and Y is left out altogether.  Why?  Because we are *modeling treatment assignment*. This creates a matchit object.

```{r}
m_cem <- matchit(formula = Treatment ~ X.0 + X.1 + X.2 + X.3 + X.4,
                 data = d,
                 cutpoints = list(X.0 = 4, X.1 = 4, X.2 = 4, X.3 = 4, X.4 = 4), 
                 # you could try = "sturges" for automatic bin selection
                 method = "cem")

m_cem
```

701 matched units. 

If we made the categories coarser we would get more matches---but at the cost of having less exact matches. There is an auto setting for the cutpoints --- "sturges" --- that will be more or less satisfactory depending on the data.  In the code above, I am manually inputting the bin numbers based on intuition about bin granularity and number of matches.  

2. Check the matches.

```{r}
summary(m_cem)
```

Around 300 units remain unmatched and will not be part of the matched data. ESS (effective sample size) of 144.76 for control means that the control units are being reused and the 338 control units are statistically equivalent to only ~145 independent control observations.  This is fine, but it does mean we need to account for this reuse later by weighting observations in the regression.

3. Plot the matches to evaluate improvement. The goal is minimize treatment/control differences in the matched sample.

```{r}
plot(summary(m_cem))
```

In general this is an improvement:  the matched sample variables are moving towards 0.

4. Construct the matched data set. This consists in a subset of only the matched units.

```{r}
(matched_cem <- match.data(m_cem))
```

Note that only the 701 matched observations are being used here; the rest have been discarded.

Two columns have been added:  weights and subclass (the coarsened bin).

    - Subclass represents a unique combination of the categories created by binning.
    - Weight for treated (if 1 treated is being matched to multiple controls) = 1
    - Weight for control is (# treated in subclass) / (# control in subclass)
    - The weights balance the number of treated and control in each subclass

5. Do the analysis using the matched data.  After matching the outcome model specification does not matter that much. You could just compare means as in an experiment (depending on how good the matches are) or -- best practice -- fit a model with all the predictors to do a final correction.

```{r}
lm(Y ~ Treatment + X.0 + X.1 + X.2 + X.3 + X.4, 
   weights = weights, # this is necessary because of number differences in subclass
   data = matched_cem) |>
  summary()
```

### Full matching 

Let's see if we can improve this using more of the data, switching from a selection method (such as CEM) to a weighting method. The method will be full matching, for which the default is PS matching. 

```{r}
# 1. Create the matchit object with glm as the (default) method for estimating propensity scores
m_full <- matchit(formula = Treatment ~ X.0 + X.1 + X.2 + X.3 + X.4,
                 data = d,
                 method = "full") # method = "quick" is an optimized version of "full"

# 2. Evaluate balance
summary(m_full)

plot(summary(m_full))

# 3. Create the matched data
(matched_full <- match.data(m_full))

# 4. Do the analysis using the weights that have been added to the matched data
lm(Y ~ Treatment + X.0 + X.1 + X.2 + X.3 + X.4,  
   data = matched_full,  
   weights = weights) |>
  summary()
```

We have retained all the data but are using the weights created by full matching in the regression equation.  The estimate of the treatment effect is .88 compared to the defined tau of .5.

### Inverse propensity score weighting (IPW)

Remember the formula for IPW:

- 1/ps for treatment
- 1/(1 - ps) for controls

We will use the inverse propensity scores as weights in regression as we did above for the matched data created with full matching. For this we don't need a package.

```{r}
# 1. Estimate propensity scores
ps_model <- glm(Treatment ~ X.0 + X.1 + X.2 + X.3 + X.4, 
                family = "binomial", 
                data = d) 

# 2. Predict probabilities.  Use type = "response".
d$ps <- predict(ps_model, type = "response") 
  
# 3. Calculate IPW
d <- d |>
  mutate(ipw = ifelse(Treatment == 1, 1/ps, 1/(1 - ps)))

# 4. Check overlap of propensity scores for trimming
ggplot(d, aes(ps)) +
  geom_histogram() +
  facet_wrap(~Treatment, ncol = 1)

# or 

summary(filter(d, Treatment == 1)$ps)
summary(filter(d, Treatment == 0)$ps)

# 5. Estimate effects using ipw as weights but trim out the lowest and highest ps
lm(Y ~ Treatment + X.0 + X.1 + X.2 + X.3 + X.4,  
   weights = ipw, 
   data = filter(d, ps > .06 & ps < .95)) |>
  summary()

```

Not bad! .64.

## Marginal Effects package

The authors of MatchIt recommend using G-Computation as implemented in the Marginal Effects package for optimal estimation after matching.  See the book that accompanies the package, *Model to Meaning*: https://marginaleffects.com/. (Here is the Matchit package vignette on using G-Computation: https://cran.r-project.org/web/packages/MatchIt/vignettes/estimating-effects.html.)

From the Introduction:  "Our world is complex. To make sense of it, data analysts routinely fit sophisticated statistical or machine learning models. Interpreting the results produced by such models can be challenging, and researchers often struggle to communicate their findings to colleagues and stakeholders. Model to Meaning is a book designed to bridge that gap. It is a practical guide for anyone who needs to translate model outputs into accurate insights that are accessible to a wide audience."

Above we used simple outcome models to estimate ATE.  But best practice with matching is actually to use a *complex* outcome model that captures all the relationships and extracts all possible signal from the data. However, the problem with complex models is that it can be hard to reconstruct effects. That is the purpose of G-computation. *It translates the complex model back to a simple, interpretable ATE.*

Example:

```{r}
# 1. fit a model with treatment - covariate interactions
model <- lm(Y ~ Treatment * X.0 * X.1 * X.2 * X.3 * X.4,  
            # or it could be less complex such as:
            # Treatment * (X.0 + X.1 + X.2 + X.3 + X.4)
            data = matched_full,  
            weights = weights) 

summary(model) # very difficult to know what the ATE is!

# 2. Use marginal effects package to back out the treatment effect from this complicated model
avg_comparisons(model,
                variables = "Treatment", #  identify the treatment variable
                wts = "ipw")$estimate # extract the estimate

```

We could use G-computation to potentially improve the IPW estimate because it allows us to extract ATE from a very complex outcome model.

```{r}
(ipw_model <- lm(Y ~ Treatment * X.0 * X.1 * X.2 * X.3 * X.4,  
   weights = ipw, 
   data = filter(d, ps > .06 & ps < .95))) |>
  summary()

avg_comparisons(ipw_model,
                variables = "Treatment", 
                wts = "ipw")$estimate
```

This is very close to ground truth.

The strategy of creating an assignment model for matching then using the resulting matched sample in a complex outcome model is called *doubly robust*. ATE estimates can be relatively accurate if the assignment model is good, even when the outcome model is misspecified. Estimates can also be relatively accurate if the outcome model is good, but the assignment model is misspecified.

## Bootstrapping

There are multiple sources of uncertainty for both IPW and full matching with G-Computation. There are two models operating in both cases, an assignment model and an outcome model. It is difficult to analytically compute standard errors in such cases. The bootstrap (also called resampling) offers a solution. It is a simple computational method for estimating the standard error of any metric. 

Steps:

1. Take a bootstrap sample by sampling with replacement.
2. Calculate the metric with the bootstrap sample.
3. Repeat many times -- 500-1000x depending on computational/time limits.
4. Use the resulting vector of simulated metrics to calculate a standard error or, more simply, the 95% confidence interval of the metric.

First, what is a bootstrap sample exactly? Here's a quick demo:

```{r}
v <- c(1, 2, 3, 4, 5)

sample(x = v, size = length(v), replace = T)
```

The bootstrap sample is the magic ingredient in bootstrapping. 

Here is an example of the so-called *non-parametric bootstrap*. Note that all the data operations need to be completed with the bootstrap sample, which varies with every loop. The goal is to evaluate statistical significance for ATE when there are multiple sources of uncertainty, as in matching with 1) an assignment model and 2) an outcome model.

Let's work with our best approach so far:  IPW.

```{r}

# initialize the vector of simulated estimates
boot_dist <- NA

for(i in 1:500){

  # initialize the bootstrap sample
  boot_samp <- slice_sample(.data = d, prop = 1, replace = T) # check ?slice_sample

  # Estimate propensity scores
  ps_model <- glm(Treatment ~ X.0 + X.1 + X.2 + X.3 + X.4, 
                family = "binomial", 
                data = boot_samp) # use the bootstrap sample here
  
  # Predict probabilities.  Use type = "response".
  boot_samp$ps <- predict(ps_model, type = "response") 
  
  # Calculate IPW
  boot_samp <- boot_samp |>
    mutate(ipw = ifelse(Treatment == 1, 1/ps, 1/(1 - ps)))
  
  # Estimate effects using ipw as weights but trim out the lowest and highest ps
  fit <- lm(Y ~ Treatment * X.0 * X.1 * X.2 * X.3 * X.4,  
            weights = ipw, 
            data = filter(boot_samp, ps > .05 & ps < .95))  # make a guess about trimming
  
  # Store estimated ATE
  boot_dist[i] <- avg_comparisons(fit, 
                                variables = "Treatment",
                                wts = "ipw")$estimate 

  print(i) # make sure to comment this out before compiling!
}

# non-parametric bootstrap estimate:
quantile(boot_dist, probs = c(.025, .5, .975)) 

# There is also a parametric bootstrap that uses SD as SE
se <- sd(boot_dist)
mean(boot_dist) - 1.96 * se # lower bound
mean(boot_dist) + 1.96 * se # upper bound


```



## Nextech Case

"Philip Mehta, a data scientist at Nextech, a large multinational manufacturer of computer and mobile-related hardware, is tasked with evaluating the effectiveness of a recently implemented management training program designed to help newly promoted managers transition from individual contributors to effective leaders. Despite the program's careful design and the head of HR's insistence on a randomized controlled trial, non-compliance among participants led to self-selection bias, confounding the results. To overcome this challenge and provide a clear, easily interpretable assessment, Philip decides to use matching, a design-based method for causal inference that aims to mimic the simplicity and credibility of a randomized experiment."

Let's take a look at the data:

```{r}
n <- read_csv("https://raw.githubusercontent.com/jefftwebb/data/main/management_training.csv") 

n
```

